---
home: true
search: false
navbar: true
sidebar: false
sidebarDepth: 2
heroImage: ./hero.jpg
heroText: Fabian-Robert St√∂ter
social: 
  <a href="https://twitter.com/faroit" alt="Twitter"><i class="fab fa-twitter fa-3x"></i></a> 
  <a href="https://github.com/faroit" alt="Github"><i class="fab fa-github fa-3x"></i></a>
  <a href="https://scholar.google.com/citations?user=7HsSdqwAAAAJ&hl=en" alt="Google Scholar"><i class="fas fa-graduation-cap fa-3x"></i></a>
  <a href="mailto:fabian-robert.stoter@inria.fr" alt="Email"><i class="fas fa-envelope fa-3x"></i></a>
  <a href="https://orcid.org/0000-0002-2534-1165" alt="Orcid"><i class="fab fa-orcid fa-3x"></i></a>
tagline: Audio-AI Researcher, Inria, Montpellier

features:
  - title: Audio-AI
    fa: fad fa-head-side-brain
    color: '#36A8A7'
    details: I work on deep learning architectures for audio related task. I am specifally interested in the task of source count estimation and <a href="https://github.io">music separation</a>.
  - title: Music Signal Processing
    fa: fas fa-music
    color: '#F27405'
    details: Interested in 
  - title: Eco-ML
    fa: fad fa-leaf
    color: '#88A61F'
    details: I am involved in the citizen science project <a href="https://plantnet.org">Pl@antNet</a> dealing with large scale plant classification as part of the <a href="https://www.cos4cloud-eosc.eu">Cos4Cloud</a> üá™üá∫ project. I am also working on ML for ecoacoustics, analyzing sounds from <a href="https://audiolog.fr">mobile audio loggers</a> deployed on ü¶ì in the wild.
footer: Made with VuePress
---

# CV

<img src="frs.jpg" width="86" style="float:left; margin-right: 1em;"> Since 2018, I am a postdoctoral researcher at the [Scientic Data Management team (Zenith)](http://www-sop.inria.fr/teams/zenith/pmwiki/pmwiki.php/Main/HomePage) at Inria in Montpellier, France. I did my Ph.D (Dr.-Ing.) at the [International Audio Laboratories Erlangen](https://www.audiolabs-erlangen.de/) in Germany supervised by [Bernd Edler](https://www.audiolabs-erlangen.de/fau/professor/edler). My dissertation titled __Separation and Count Estimation for Audio Sources Overlapping in Time and Frequency__ can be viewed [here](https://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/13114). Before, I graduated in electrical engineering / communication engineering from the [University of Hannover, Germany](https://www.uni-hannover.de). An extended CV is available [here](#).

# Current Research Interests 

- __Deep learning on data hubs__: I am interested multi-modal auto-encoders that can learn the relations between the different modalities, so as to reconstruct or enhance missing or degraded data. Also, I see my work on multistore and heterogeneous heritage datasets are the natural settings for continuing my past research on generative modelling.

- __User-centered AI for audio data__: I want to develop new methods and tools for users with domain knowledge to deliver interpretable audio models. Furthermore, _evaluation_ of audio processing tasks is often done in a computational manner, due to the lack of expertise from signal processing researchers in organizing perceptual evaluation campaigns. This leads to the non-acceptance of technologies by (possibly expert) users.

- __Ecological machine-learning__: I want to play a role in reducing the carbon footprint of my work. Reducing the size of datasets speeds up training and therefore saves energy. Reducing the computational complexity of models is an active research topic, with strongly investigated ideas like quantization, pruning or compression. Inspired by current trends in [differentiable signal processing](), I want to focus on converting large deep models to embedded energetic-efÔ¨Åcient Ô¨Ålters so that they can be deployed on edge devices.

# Supervision

- Lucas Mathieu, Research Internship (Master), (AgroParisTech, France) <Badge text="Ongoing" type="success"/>
- [Wolfgang Mack](https://www.audiolabs-erlangen.de/fau/assistant/mack), Master Thesis (FAU Erlangen-N√ºrnberg, Germany)
- Erik Johnson, [DAAD](https://www.daad.de/rise/en/)rsearch internship (Carleton University, Canada)
- [Nils Werner](https://nils-werner.github.io/), Master Thesis, (FAU Erlangen-N√ºrnberg, Germany)
- Jeremy Hunt,  [DAAD](https://www.daad.de/rise/en/) research internship, (Rice University, USA)
- Ercan Berkan, Master Thesis, (Bilkent University, Turkey)  
- Shujie Guo, Research Internship, (FAU Erlangen-N√ºrnberg, Germany)
- [Aravindh Krishnamoorty](https://www.idc.tf.fau.de/person/aravindh-krishnamoorthy), Internship
- Bufei Liu, Research Internship (Shanghai University, China)
<!-- - Qiao Wang -->

# Scientific Service

* __Journals:__ IEEE [Transaction in Audio, Speech and Language Processing](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6570655), [Signal Processing Letters](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=97), [EURASIP](https://www.eurasip.org/), [Journal of Open Source Software](https://github.com/search?q=repo%3Aopenjournals%2Fjoss-reviews+label%3Areview+assignee%3Afaroit&type=Issues&ref=advsearch&l=&l=)
* __Conferences:__ [ICASSP](http://ieeeicassp.org/), [EUSIPCO](https://eusipco2020.org/), [DAFx](https://www.dafx.de/), [ISMIR](https://www.ismir.net).

# Teaching

## Graduate Programs

* __2020__: [Research Internship](https://www.polytech.umontpellier.fr/partenariats/stages-et-projets) (Master, Stage 5), PolyTech Montpellier
* __2018, 2019__: Introduction to Deep Learning, Master 2, PolyTech Montpellier
* __2016__: [Reproducible Audio Research Seminar](https://github.com/audiolabs/APSRR-2016), University of Erlangen (Germany)
* __2014-2016__: Multimedia Programming , Highschool Students, University of Erlangen (Germany)
* __2013-2016__: Lab Course, _Statistical Methods for Audio Experiments_, Master Students, University of Erlangen (Germany) [Course Material](https://www.audiolabs-erlangen.de/content/05-fau/professor/00-mueller/02-teaching/2016s_apl/LabCourse_StatsMethods.pdf).

## Talks

* __2019__: Invited talk at a conference [‚ÄúDeep learning: From theory to applications‚Äù](https://www.lebesgue.fr/content/sem2018-deeplearning) titled ‚ÄúDeep learning for music unmixing‚Äù. [Video](https://www.lebesgue.fr/video/2879) [Slides]()
* __2019__: Tutorial at [EUSIPCO 2019](http://eusipco2019.org/): _"Deep learning for music separation"_. <a href="https://sigsep.github.io/tutorials/"><i class="fad fa-presentation"></i> Slides</a> <a href="https://sigsep.github.io/tutorials/"><i class="fad fa-browser"></i> Website</a>
* __2018__: Tutorial at [ISMIR 2019](http://ismir2018.ircam.fr/pages/events-tutorial-01.html): _"Music Separation with DNNs: Making It Work"_. <a href="https://sigsep.github.io/tutorials/"><i class="fad fa-presentation"></i> Slides</a> <a href="https://sigsep.github.io/tutorials/"><i class="fad fa-browser"></i> Website</a>

# Software

## <img src='./images/pytorchlogo.svg' width="20rem"> open-unmix

<iframe width="640" height="480" src="https://www.youtube-nocookie.com/embed/IxLnoy-GzqI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" class="" allowfullscreen></iframe>

Open-Unmix, a deep neural network reference implementation ([PyTorch](https://github.com/sigsep/open-unmix-pytorch) and [NNabla](https://github.com/sigsep/open-unmix-nnabla)) for music source separation, applicable for researchers, audio engineers and artists. Open-Unmix provides ready-to-use models that allow users to separate pop music into four stems: vocals, drums, bass and the remaining other instruments. 

<a href="https://github.com/sigsep/open-unmix-pytorch"><i class="fab fa-github"></i> Code</a> <a href="https://sigsep.github.io/open-unmix/"><i class="fad fa-browser"></i> Website/Demo</a>

## <i class="fab fa-python"></i> musdb

<img src="https://sigsep.github.io/assets/img/musheader.41c6bf29.png" width="640">

A python package to parse and process the [MUSDB18 dataset](https://sigsep.github.io/musdb), the largest open access dataset for music source separation. The tool was originally developed for the [Music Separation task](sisec18.unmix.app) as part of the [Signal Separation Evaluation Campaign (SISEC)](https://sisec.inria.fr/). Using `musdb` users can quickly iterate over multi-track music datasets. In just three lines of code a subset of the MUSDB18 is automatically downloaded and can be parsed:

```python
import musdb
mus = musdb.DB(download=True)
for track in mus:
    train(track.audio, track.targets['vocals'].audio)
```

<a href="https://github.com/sigsep/sigsep-mus-db"><i class="fab fa-github"></i> Code</a>


## <i class="fab fa-python"></i> museval

## <i class="fab fa-python"></i> stempeg

## <i class="fab fa-python"></i> trackswitch.js

## <i class="fab fa-python"></i> webMUSHRA

## <i class="fab fa-python"></i> norbert



<a href="https://github.com/sigsep/norbert"><i class="fab fa-github"></i> Code</a> 
# Open-Ressources

## <i class="fab fa-python"></i> awesome-scientific-python-audio
## <i class="fab fa-python"></i> reproducible research for audio


# Datasets

## <img src='./images/sigsep.png' width="20rem">  MUSDB

2017, 150 tracks (~10h) Multitrack Music 
| [MUSDB18-HQ](/datasets/musdb.md) | 2019     | 150 tracks (~10h)       | Multitrack Music Dataset          | 


* [MUSDB18](/datasets/musdb.md) | 2017
* [MUSDB18-HQ](/datasets/musdb.md) | 2019


# Hackathon-Projects

## Chromeleiter

## Magiclock 

<img src="https://cloud.githubusercontent.com/assets/72940/16177370/e492626a-362a-11e6-9f66-2291040f98c1.gif" width="640">

Magiclock is an OS X app that uses haptic feedback (also called Taptic Engine‚Ñ¢) to give you the MIDI clock beat within your Magic Trackpad.

[Code](https://github.com/faroit/magiclock)

## Git-Push

<img src="https://camo.githubusercontent.com/c4eeb051705f27dd1d531ace4c540e8fc3954d36/687474703a2f2f692e696d6775722e636f6d2f446453686271322e6a7067" height="280"  align="left">

<img src="https://user-images.githubusercontent.com/72940/28498777-0454202e-6fa6-11e7-8a3c-9c85506013fa.gif" height="280">

* https://github.com/RocketScienceAbteilung/git-wig

lorem akjsjd lkasjd lkasjd lkasjd lksaj 
sadjkf sdjlkfj salkdj flkajs dfjalksjd f


## DeepFandom  <Badge text="1st Prize" type="success"/>

<iframe width="320" height="180" src="https://www.youtube-nocookie.com/embed/uPb56-UfFRI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

DeepFandom is a deep learning model that learns the [Soundcloud](soundcloud.com) comments and predicts what _YOUR_ track could get as comments and where they are positioned on the waveform. [Website](https://devpost.com/software/deep-fandom)

# Publications

<publications zotero_id="6408178"></publications>
