---
home: true
search: false
navbar: true
sidebar: false
sidebarDepth: 2
heroImage: ./hero2.jpg
heroText: Fabian-Robert St√∂ter
social:
  <a rel="me" href="https://sigmoid.social/@faro" alt="Mastodon"><i class="fab fa-mastodon fa-2x"></i></a>
  <a href="https://github.com/faroit" alt="Github"><i class="fab fa-github fa-2x"></i></a>
  <a href="https://scholar.google.com/citations?user=7HsSdqwAAAAJ&hl=en" alt="Google Scholar"><i class="fas fa-graduation-cap fa-2x"></i></a>
  <a href="mailto:fabian-robert.stoter@inria.fr" alt="Email"><i class="fas fa-envelope fa-2x"></i></a>
  <a href="https://orcid.org/0000-0002-2534-1165" alt="Orcid"><i class="fab fa-orcid fa-2x"></i></a>
tagline: Audio-AI Researcher at Audioshake.ai, Montpellier

features:
  - title: Music Processing
    fa: fas fa-music
    color: "#F27405"
    details: I have a background in digital signal processing (DSP) and have worked on a wide range of audio and related tasks, including speech- and audio processing, music analysis and music information retrieval.
  - title: Audio-AI
    fa: fas fa-brain
    color: "#36A8A7"
    details: I have a profound understanding of deep audio ml. I am specifically interested in the tasks of <a href="https://github.com/faroit/CountNet"> source count estimation</a> and <a href="https://sigsep.github.io">audio source separation</a>. I am leading the research team at <a href="https://audioshake.ai">Audioshake.ai</a> that created the best performing music separation model.
  - title: Eco-ML
    fa: fab fa-pagelines
    color: "#88A61F"
    details: I am involved in <a href="https://plantnet.org">Pl@ntNet</a> as part of <a href="https://www.cos4cloud-eosc.eu">Cos4Cloud üá™üá∫</a> citizen science project. I am also working on ML for ecoacoustics, analyzing sounds of ü¶ì using <a href="https://audiolog.fr">mobile audio loggers</a>.
footer: <a href="https://github.com/faroit/website"><i class="fab fa-github"></i> Code for this Website</a>
---

# About me

<img src="frs.jpg" width="86" style="float:left; margin-right: 1em;"> Since 2021, I'm head of research at [audioshake.ai](https://www.audioshake.ai) working on audio source separation. Before, I was a postdoctoral researcher at the [Inria and University of Montpellier](http://www-sop.inria.fr/teams/zenith/pmwiki/pmwiki.php/Main/HomePage), France. I did my Ph.D (Dr.-Ing.) at the [International Audio Laboratories Erlangen](https://www.audiolabs-erlangen.de/) (is a joint institution of [Fraunhofer IIS](https://www.iis.fraunhofer.de) and [FAU Erlangen-N√ºrnberg](https://fau.de)) in Germany supervised by [Bernd Edler](https://www.audiolabs-erlangen.de/fau/professor/edler). My dissertation titled **¬´Separation and Count Estimation for Audio Sources Overlapping in Time and Frequency¬ª** can be viewed [<i class="fas fa-file-pdf"></i> here](https://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/13114). Before, I graduated in electrical engineering / communication engineering from the [University of Hannover, Germany](https://www.uni-hannover.de). An extended CV is available [<i class="fas fa-file-pdf"></i> here](https://github.com/faroit/resume/releases/download/v1.0.2/stoeter_resume.pdf).

## Current Research Interests

- **Deep learning on data hubs**: I am interested multi-modal auto-encoders that can learn the relations between the different modalities to reconstruct or enhance missing or degraded data. Also, I work on multistore and heterogeneous heritage datasets.

- **User-centered AI for audio data**: I want to develop new methods and tools for users with domain knowledge to deliver interpretable audio models. Furthermore, _evaluation_ of audio processing tasks is often done in a computational manner, due to the lack of expertise from signal processing researchers in organizing perceptual evaluation campaigns.

- **Ecological machine-learning**: I want to play a role in reducing the carbon footprint of my work. Reducing the size of datasets speeds up training and therefore saves energy. Reducing the computational complexity of models is an active research topic, with strongly investigated ideas like quantization, pruning or compression. Inspired by current trends in differentiable signal processing, I want to convert deep models so that they can be deployed on edge devices.

## Press/Media Interviews

- <i class="fas fa-podcast"></i> **02/2023** ["l'intelligence artificielle et du droit d'auteur", Radio-Canada (French)](https://ici.radio-canada.ca/ohdio/premiere/emissions/jusquau-bout/episodes/683554/rattrapage-du-vendredi-27-janvier-2023)
- <i class="fas fa-podcast"></i> **12/2022** ["Jahresr√ºckblick und Vorausschau: KI Musik und Metaverse", Deutschlandfunk Kultur (German)](https://www.deutschlandfunkkultur.de/jahresrueckblick-und-vorausschau-ki-musik-und-metaverse-dlf-kultur-e70bf447-100.html)
- <i class="fas fa-podcast"></i> **02/2022** ["L'intelligence artificielle au profit des stems musicaux", Radio-Canada (French)](https://ici.radio-canada.ca/ohdio/premiere/emissions/jusquau-bout/episodes/605268/rattrapage-du-vendredi-11-fevrier-2022/1)
- <i class="fas fa-podcast"></i> **12/2021** ["Recycling von Songs: Wie KI neue Musik generiert", Deutschlandfunk Kultur (German)](https://www.deutschlandfunkkultur.de/recycling-von-songs-wie-ki-neue-musik-generiert-dlf-kultur-90e01124-100.html)

# Scientific Service

## Student Supervision

- Johannes Imort, Master student, RWTH Aachen (Germany), Internship _"Sound Activity Detection"_ (ongoing).
- Yeong-Seok Jeong and [Jinsung Kim](https://onedas.github.io/), Master students, Korea University, (Winter 2022/2023) Internship on _"Unsupervised Music Separation"_ (Summer 2022).
- Michael T√§nzer, PhD student, Fraunhofer IDMT (Germany), (Summer 2021), Internship on audio tagging.
- [Lucas Mathieu](https://synergy.st-andrews.ac.uk/cbd/person/lm354/), Master student, AgroParistech (France), Master thesis _"Listening to the Wild"_ (03/2020). Theoretical research on self-supervised learning using data from animal-born loggers ([MUSE project](https://muse.edu.umontpellier.fr/)). Lucas was accepted as a PhD student after master thesis.
- Clara Jacintho and Delton Vaz, Bachelor Thesis, PolyTech Montpellier (France), _"Machine Learning for Audio on the Web"_ (12/2019). Research on web based separation architectures. Resulted in a paper submitted to the [Web Audio Conference 2021](https://webaudioconf2021.com).
- [Wolfgang Mack](https://www.audiolabs-erlangen.de/fau/assistant/mack), Master Thesis (FAU Erlangen-N√ºrnberg, Germany), _"Investigations on Speaker Separation using Embeddings obtained by Deep Learning"_, (05/2017), Wolfgang was accepted as PhD student after master thesis.
- [Erik Johnson](https://ca.linkedin.com/in/ecmjohnson), [DAAD](https://www.daad.de/rise/en/) Research internship, (Carleton University, Canada), [_"Open-Source Implementation of Multichannel BSSEval in Python"_](https://github.com/craffel/mir_eval/pull/199) (03/2014).
- [Nils Werner](https://nils-werner.github.io/), Master Thesis, (FAU Erlangen-N√ºrnberg, Germany), _"Parameter Estimation for Time-Varying Harmonic Audio Signals"_, (02/2014), Nils was accepted as PhD student after master thesis.
- Jeremy Hunt, [DAAD](https://www.daad.de/rise/en/) research internship, (Rice University, USA)
- Bufei Liu, Master, Research Internship (Shanghai University, China), 2014.
- [Aravindh Krishnamoorty](https://www.idc.tf.fau.de/person/aravindh-krishnamoorthy), Master, Internship, 2014
- Ercan Berkan, Master Thesis, (Bilkent University, Turkey), _"Music Instrument Source Separation"_, 3/2013
- Shujie Guo, Master, Research Internship, (FAU Erlangen-N√ºrnberg, Germany)

## Reviewing

- **Journals:** IEEE [Transaction in Audio, Speech and Language Processing](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6570655), [Signal Processing Letters](https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=97), [EURASIP](https://www.eurasip.org/), [Journal of Open Source Software](https://joss.theoj.org/papers/reviewed_by/@faroit)
- **Conferences:** [ICASSP](http://ieeeicassp.org/), [EUSIPCO](https://eusipco2020.org/), [DAFx](https://www.dafx.de/), [ISMIR](https://www.ismir.net).

## Editing

- **Journals**: Topic Editor for ML-Audio for the [Journal of Open Source Software](https://joss.theoj.org/papers/edited_by/@faroit).

## Teaching

### Graduate Programs

- **2021**: [Lecture: Selected Topics in Deep Learning for Audio, Speech, and Music Processing](https://audiolabs-erlangen.de/fau/professor/mueller/teaching/2021s_dla), Music Source Separation, University of Erlangen (Germany).
- **2020**: [Research Internship](https://www.polytech.umontpellier.fr/partenariats/stages-et-projets) (Master, Stage 5), PolyTech Montpellier
- **2018, 2019**: Introduction to Deep Learning, Master 2, PolyTech Montpellier
- **2016**: [Reproducible Audio Research Seminar](https://github.com/audiolabs/APSRR-2016), University of Erlangen (Germany)
- **2014-2016**: Multimedia Programming , Highschool Students, University of Erlangen (Germany)
- **2013-2016**: Lab Course, _Statistical Methods for Audio Experiments_, Master Students, University of Erlangen (Germany) [<i class="fas fa-file-pdf"></i> Course Material](https://www.audiolabs-erlangen.de/content/05-fau/professor/00-mueller/02-teaching/2016s_apl/LabCourse_StatsMethods.pdf).

### Talks

- **2023**: "Music Source Separation: Is it solved yet?", ParisTech, Paris (France)
- **2020**: Invited talk at AES Symposium ["AES Virtual Symposium: Applications of Machine Learning in Audio"](https://www.aes.org/events/2020/learning/) titled "Current Trends in Audio Source Separation". [Slides (PDF)](https://sigsep.github.io/AES2020_CurrentTrendsInSourceSeparation.pdf) [Video](https://www.youtube.com/watch?v=AB-F2JmI9U4)
- **2019**: Invited talk at a conference [‚ÄúDeep learning: From theory to applications‚Äù](https://www.lebesgue.fr/content/sem2018-deeplearning) titled ‚ÄúDeep learning for music unmixing‚Äù. [Video](https://www.lebesgue.fr/video/2879) [Slides]()
- **2019**: Tutorial at [EUSIPCO 2019](http://eusipco2019.org/): _"Deep learning for music separation"_. <a href="https://sigsep.github.io/tutorials/"><i class="fad fa-presentation"></i> Slides</a> <a href="https://sigsep.github.io/tutorials/"><i class="fas fa-link"></i> Website</a>
- **2018**: Tutorial at [ISMIR 2019](http://ismir2018.ircam.fr/pages/events-tutorial-01.html): _"Music Separation with DNNs: Making It Work"_. <a href="https://sigsep.github.io/tutorials/"><i class="fad fa-presentation"></i> Slides</a> <a href="https://sigsep.github.io/tutorials/"><i class="fas fa-link"></i> Website</a>

### Other Ressources

- [sigsep.io](https://sigsep.github.io) - Open ressources for music separation.
- [awesome-scientific-python-audio](https://github.com/faroit/awesome-python-scientific-audio) - Curated list of python packages for scientific research in audio.

# Software

## <img src='./images/pytorchlogo.svg' width="20rem"> open-unmix <a href="https://devpost.com/software/open-unmix"><Badge text="Winner: Pytorch Global Hackathon 2019" type="success"/></a>

<iframe width="100%" height="300rem" src="https://www.youtube-nocookie.com/embed/IxLnoy-GzqI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" class="" allowfullscreen></iframe>

Open-Unmix, a deep neural network reference implementation ([PyTorch](https://github.com/sigsep/open-unmix-pytorch) and [NNabla](https://github.com/sigsep/open-unmix-nnabla)) for music source separation, applicable for researchers, audio engineers and artists. Open-Unmix provides ready-to-use models that allow users to separate pop music into four stems: vocals, drums, bass and the remaining other instruments.

Demo Separations on [MUSDB18](https://sigsep.github.io/musdb) Dataset:

<iframe width="100%" height="490rem" src="https://d2cowzs755i94n.cloudfront.net" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" class="" allowfullscreen></iframe>

<a class="button" href="https://sigsep.github.io/open-unmix/"><i class="fas fa-link"></i> Website/Demo</a>
<a class="button" href="https://github.com/sigsep/open-unmix-pytorch"><i class="fab fa-github"></i> Code</a>
<a class="button" href="https://joss.theoj.org/papers/10.21105/joss.01667"><i class="fa fa-file-pdf"></i> Paper</a>
<a class="button" href="https://anr.fr/fr/actualites-de-lanr/details/news/open-unmix-un-logiciel-open-source-issu-du-projet-anr-kamoulox-pour-demixer-la-musique/"><i class="fas fa-link"></i> ANR Blog (french)</a>
<a class="button" href="https://devpost.com/software/open-unmix"><i class="fad fa-rocket-launch"></i> Pytorch Hackathon</a>

## CountNet

<video width="100%" controls>
  <source src="https://www.audiolabs-erlangen.de/content/resources/00-2017-CountNet/rnn_demo.mp4" type="video/mp4">
  Your browser does not support the video tag.
</video>

CountNet is a deep learning model that estimates the number of concurrent speakers from single channel speech mixtures. This task is a mandatory Ô¨Årst step to address any realistic ‚Äúcocktail-party‚Äù scenario. It has various audio-based applications such as blind source separation, speaker diarisation, and audio surveillance.

<a class="button" href="https://github.com/faroit/countnet"><i class="fab fa-github"></i> code</a>

## <i class="fab fa-python"></i> musdb + museval

A python package to parse and process the [MUSDB18 dataset](https://sigsep.github.io/musdb), the largest open access dataset for music source separation. The tool was originally developed for the [Music Separation task](sisec18.unmix.app) as part of the [Signal Separation Evaluation Campaign (SISEC)](https://sisec.inria.fr/).

Using `musdb` users can quickly iterate over multi-track music datasets. In just three lines of code a subset of the MUSDB18 is automatically downloaded and can be parsed:

```python
import musdb
mus = musdb.DB(download=True)
for track in mus:
    train(track.audio, track.targets['vocals'].audio)
```

Now, given a trained model, evaluation can simply be performed using **museval**

```python
import museval
for track in mus:
    estimates = predict(track)  # model outputs dict
    scores = museval.eval_mus_track(track, estimates)
    print(scores)
```

<a class="button" href="https://github.com/sigsep/sigsep-mus-db"><i class="fab fa-github"></i> musdb</a>
<a class="button" href="https://github.com/sigsep/sigsep-mus-eval"><i class="fab fa-github"></i> museval</a>

## Hackathon Projects

### DeMask <Badge text="1st Place" type="success"/>

**Event**: 2020 PyTorch Summer Hackathon ‚Äì
**Collaborators**: Manuel Pariente, Samuele Cornell, Michel Olvera, Jonas Haag

<iframe width="100%" height="300rem" src="https://www.youtube-nocookie.com/embed/QLf10Uqu8Yk" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

DeMask is an end-to-end model for enhancing speech while wearing face masks ‚Äî offering a clear benefit during times when face masks are mandatory in many spaces and for workers who wear face masks on the job. Built with Asteroid, a PyTorch-based audio source separation toolkit, DeMask is trained to recognize distortions in speech created by the muffling from face masks and to adjust the speech to make it sound clearer.

<a class="button" href="https://devpost.com/software/asteroid-the-pytorch-based-source-separation-toolkit"><i class="fas fa-link"></i> DevPost Website</a>

### `git wig` <Badge text="Winner" type="success"/>

**Event**: 2015 Midi-Hackday Berlin,
**Collaborators**: [Nils Werner](https://nils-werner.github.io/), [Patricio-Lopez Serrano](https://www.audiolabs-erlangen.de/fau/assistant/lopez)

<img src="https://camo.githubusercontent.com/c4eeb051705f27dd1d531ace4c540e8fc3954d36/687474703a2f2f692e696d6775722e636f6d2f446453686271322e6a7067" height="238rem"  align="left">

<img src="https://user-images.githubusercontent.com/72940/28498777-0454202e-6fa6-11e7-8a3c-9c85506013fa.gif" height="238rem">

Why can't we have version on control for making music? In this hack, we merged `git` with a terminal based music sequencer, calling it `git wig`. We also created a suitable, diffable sequencer format to compose music. Finally, we realized `git push` by bringing this feature into a hardware controller.

<a class="button" href="https://github.com/RocketScienceAbteilung/git-grid"><i class="fab fa-github"></i> git grid</a>
<a class="button" href="https://github.com/RocketScienceAbteilung/git-wig"><i class="fab fa-github"></i> git wig</a>

### DeepFandom <Badge text="1st Place" type="success"/>

**Event:** 2016 Music Hackday Berlin.
**Collaborators**: [Patricio-Lopez Serrano](https://www.audiolabs-erlangen.de/fau/assistant/lopez)

<iframe width="100%" height="300rem" src="https://www.youtube-nocookie.com/embed/uPb56-UfFRI" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

DeepFandom is a deep learning model that learns the [Soundcloud](soundcloud.com) comments and predicts what _YOUR_ track could get as comments and where they are positioned on the waveform.

<a class="button" href="https://devpost.com/software/deep-fandom"><i class="fas fa-link"></i> Website</a>

### Magiclock

<!--  -->
<img src="https://cloud.githubusercontent.com/assets/72940/16177370/e492626a-362a-11e6-9f66-2291040f98c1.gif" width="100%">

Magiclock is an macOS application that uses haptic feedback (also called Taptic Engine‚Ñ¢) to let you **feel** the MIDI clock beat from your Magic Trackpad.

<a class="button" href="https://github.com/faroit/magiclock"><i class="fab fa-github"></i> Code</a>

## Other Software Contributions

- <i class="fab fa-python"></i> [stempeg](https://github.com/faroit/stempeg) - read/write of STEMS multistream audio.
- <i class="fab fa-js"></i> <a href="https://github.com/audiolabs/trackswitch.js/">trackswitch.js</a> - A Versatile Web-Based Audio Player for Presenting Scientifc Results.
- <i class="fab fa-js"></i> <a href="https://github.com/audiolabs/webMUSHRA">webMUSHRA</a> - MUSHRA compliant web audio API based experiment software.
- <i class="fab fa-python"></i> <a href="https://github.com/sigsep/norbert">norbert</a> - Painless Wiener filters for audio separation.

# Datasets

## <img src='./images/sigsep.png' width="20rem"> MUSDB18

![](https://sigsep.github.io/assets/img/musheader.41c6bf29.png)

The _musdb18_ is a dataset of 150 full lengths music tracks (~10h duration) of different genres along with their isolated drums, bass, vocals and others stems. It is currently the largest, publicly available dataset used for music separation. _MUSDB18_ serves as a benchmark for music separation tasks.

<a class="button" href="https://sigsep.github.io/datasets/musdb.html"><i class="fas fa-link"></i> Website</a>
<a class="button" href="<https://paperswithcode.com/dataset/musdb18>
"><i class="fas fa-link"></i> Paperswithcode</a>

## LibriCount

![](https://www.audiolabs-erlangen.de/content/resources/00-2017-CountNet/teaser.svg)

The dataset contains a simulated cocktail party environment of [0..10] speakers, mixed with 0dB SNR from random utterances of different speakers from the LibriSpeech `CleanTest` dataset.
All recordings are of 5s durations, and all speakers are active for the most part of the recording. For each unique recording, we provide the audio wave file (16bits, 16kHz, mono) and an annotation `json` file with the same name as the recording.

<a class="button" href="https://denumerate.app"><i class="fas fa-link"></i> Listening Experiment</a>
<a class="button" href="https://zenodo.org/record/1216072"><i class="fas fa-link"></i> Download</a>

## Muserc

<iframe width="100%" height="300rem" src="https://www.youtube-nocookie.com/embed/yOKvqz2jZgM" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" class="" allowfullscreen></iframe>

A novel dataset for musical instruments where we recorded a **violin cello** that includes sensor recordings capturing the Ô¨Ånger position on the Ô¨Ångerboard which is converted into an instantaneous frequency estimate. We also included professional high-speed video camera data to capture excitations from the string at 2000 fps. All of the data is sample synchronized

<a class="button" href="https://www.audiolabs-erlangen.com/resources/muserc"><i class="fas fa-link"></i> Website</a>
<a class="button" href="https://zenodo.org/record/1560651"><i class="fas fa-link"></i> Download</a>

# Publications

<a class="button" href="https://scholar.google.com/citations?user=7HsSdqwAAAAJ&hl=en"><i class="fas fa-graduation-cap"></i> Google Scholar</a> <a class="button" href="https://www.zotero.org/faroit"><i class="fas fa-graduation-cap"></i> Zotero</a>

## Peer-Reviewed Journals

<publications zotero_id="6408178" filter="itemType=journalArticle"></publications>

## Peer-Reviewed Conferences

<publications zotero_id="6408178" filter="itemType=conferencePaper"></publications>
